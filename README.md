Матвеев Климентий Максимович 11-208
Нонская Лейсан Алишеровна 11-208
Задание 1 по Основам Информационного Поиска

за основу взята случайная статья с Хабра, дальше идет переход по ссылкам в глубину до 3 не считая начальную ссылку(обычно до 3 глубины не доходит, так как стоит ограничение в 150 страниц)
все результаты сохраняются в index.txt и results.csv второй более информативный 
все файлы сохраняются в папку downloaded_pages



Кратко попытался описать:

1. Init:
   - Создаётся объект класса WebSpider с указанным начальным URL
   - Загружаются данные о ранее скачанных страницах (если есть)
   - Создаётся очередь для обхода сайта

2. Сам обход сайта (BFS(beautiful soup) - поиск в ширину):
   - Берём первую ссылку из очереди
   - Скачиваем страницу по этой ссылке
   - Сохраняем HTML-код в файл
   - Извлекаем все ссылки со страницы
   - Добавляем новые ссылки в очередь с увеличенной глубиной
   - Повторяем до достижения цели (150 страниц) или исчерпания ссылок

3. Очеред обработки:
   Каждый элемент очереди содержит:
   - url: адрес страницы для скачивания
   - depth: глубина обхода (0, 1, 2, 3)
   - parent_url: ссылка на страницу, откуда была извлечена эта ссылка

4. Сохранение:
   - Каждая страница сохраняется как page_N.html в папку downloaded_pages
   - Информация о страницах сохраняется в двух файлах:
     * index.txt - простой список "номер ссылка"
     * results.csv - таблица с колонками: номер, ссылка, имя файла, родительская страница(это я от себя добавил, по идее не нужно)

5. Проверка повторов:
   - Используется множество visited_urls для отслеживания уже посещённых страниц
   - Ссылки проверяются на валидность (не медиафайлы, внутренние ссылки сайта)

6. Ограничения задаются в мейне:
   - Максимум страниц
   - Глубина обхода не более X уровней

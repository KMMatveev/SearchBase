Матвеев Климентий Максимович 11-208
Нонская Лейсан Алишеровна 11-208
Задание 1 по Основам Информационного Поиска

за основу взята случайная статья с Хабра, дальше идет переход по ссылкам в глубину до 3 не считая начальную ссылку(обычно до 3 глубины не доходит, так как стоит ограничение в 150 страниц)
все результаты сохраняются в index.txt и results.csv второй более информативный 
все файлы сохраняются в папку downloaded_pages



Кратко попытался описать:

1. Init:
   - Создаётся объект класса WebSpider с указанным начальным URL
   - Загружаются данные о ранее скачанных страницах (если есть)
   - Создаётся очередь для обхода сайта

2. Сам обход сайта (BFS(beautiful soup) - поиск в ширину):
   - Берём первую ссылку из очереди
   - Скачиваем страницу по этой ссылке
   - Сохраняем HTML-код в файл
   - Извлекаем все ссылки со страницы
   - Добавляем новые ссылки в очередь с увеличенной глубиной
   - Повторяем до достижения цели (150 страниц) или исчерпания ссылок

3. Очеред обработки:
   Каждый элемент очереди содержит:
   - url: адрес страницы для скачивания
   - depth: глубина обхода (0, 1, 2, 3)
   - parent_url: ссылка на страницу, откуда была извлечена эта ссылка

4. Сохранение:
   - Каждая страница сохраняется как page_N.html в папку downloaded_pages
   - Информация о страницах сохраняется в двух файлах:
     * index.txt - простой список "номер ссылка"
     * results.csv - таблица с колонками: номер, ссылка, имя файла, родительская страница(это я от себя добавил, по идее не нужно)

5. Проверка повторов:
   - Используется множество visited_urls для отслеживания уже посещённых страниц
   - Ссылки проверяются на валидность (не медиафайлы, внутренние ссылки сайта)

6. Ограничения задаются в мейне:
   - Максимум страниц
   - Глубина обхода не более X уровней

Задание 2:
7.Токенизация:
    Из каждого HTML-файла извлекается чистый текст (удаляются теги <script>, <style>, <header>, <footer>)
    Текст разбивается на токены с помощью regex
    Оставляются только слова, состоящие из букв (кириллица или латиница)
    
8.Фильтрация токенов:
    Удаляются токены короче 2 символов и длиннее 50 символов
    Удаляются стоп-слова (предлоги, союзы, частицы, местоимения)
    Удаляются чистые числа (123, 2024)
    Удаляются слова из букв и цифр (win10, v2.0, 100кг)
    Удаляются остатки разметки (</div>, {{, @#$)
    Удаляются слова с повторяющимися символами
    Все токены приводятся к нижнему регистру
    
9.Лемматизация:
    Каждый токен проходит через морфологический анализатор
    Определяется нормальная форма слова (лемма)
    Токены группируются по леммам для отслеживания словоформ

10.Сохранение результатов:
    Для каждой страницы создаётся отдельная папка: tokens_output/page_001/
    В папке сохраняются два файла:
        tokens.txt — список уникальных токенов (по одному в строке)
        lemmas.txt — леммы с токенами (формат: <лемма> <токен1> <токен2> ... <токенN>)
